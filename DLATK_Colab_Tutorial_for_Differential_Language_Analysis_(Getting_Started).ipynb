{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melinaomueller/DLATK/blob/main/DLATK_Colab_Tutorial_for_Differential_Language_Analysis_(Getting_Started).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> DLATK - Colab Tutorial for Differential Language Analysis and Prediction </h1>\n",
        "\n",
        "<br/>\n",
        "\n",
        "âœ‹ **NOTE** - You need to create a copy of this notebook before you work through it. This can be done by clicking on \"Save a copy in Drive\" option in the File menu.\n",
        "\n",
        "<br/>\n",
        "\n",
        "This tutorial covers:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16SgSLj9KkmxUX8LEeFNsq0GRE4gqvQux\" height=\"170\"/>"
      ],
      "metadata": {
        "id": "0Jf8iKr5C_nO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TnZ_cBOTGBZ"
      },
      "source": [
        "# Setup Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This required code block will setup your Colab environment for dlatk by:**\n",
        "\n",
        "1. Installing `dlatk` and its dependencies.\n",
        "2. Telling dlatk it is in colab mode by running `--colabify`.\n",
        "3. Adjusting colab's output format."
      ],
      "metadata": {
        "id": "dv5i6CNrWRz0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX7WCq5dToGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e21c15-c325-4449-d5fe-28cbf1a44d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dlatk[langid,wordcloud] in /usr/local/lib/python3.11/dist-packages (1.3.15)\n",
            "Requirement already satisfied: nltk<=3.8.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (3.8.1)\n",
            "Requirement already satisfied: numpy<=1.25.2 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.25.2)\n",
            "Requirement already satisfied: pandas<=1.5.3,>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.5.3)\n",
            "Requirement already satisfied: patsy<=0.5.6,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil<=2.8.2,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn<=1.1.3 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.1.3)\n",
            "Requirement already satisfied: scipy<=1.11.4,>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.11.4)\n",
            "Requirement already satisfied: statsmodels<=0.14.1,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (0.14.1)\n",
            "Requirement already satisfied: langid<=1.1.6,>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.1.6)\n",
            "Requirement already satisfied: wordcloud<=1.9.3,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from dlatk[langid,wordcloud]) (1.9.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<=3.8.1,>=3.1->dlatk[langid,wordcloud]) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<=3.8.1,>=3.1->dlatk[langid,wordcloud]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<=3.8.1,>=3.1->dlatk[langid,wordcloud]) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<=3.8.1,>=3.1->dlatk[langid,wordcloud]) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<=1.5.3,>=0.17.1->dlatk[langid,wordcloud]) (2025.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from patsy<=0.5.6,>=0.2.1->dlatk[langid,wordcloud]) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<=1.1.3->dlatk[langid,wordcloud]) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels<=0.14.1,>=0.5.0->dlatk[langid,wordcloud]) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud<=1.9.3,>=1.1.3->dlatk[langid,wordcloud]) (3.2.3)\n",
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-28 21:44:39\n",
            "-----\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - None\n",
            "Corpus - None\n",
            "Group ID - user_id\n",
            "-------\n",
            "Interface Runtime: 1.10 seconds\n",
            "DLATK exits with success! A good day indeed  Â¯\\_(ãƒ„)_/Â¯.\n"
          ]
        }
      ],
      "source": [
        "!pip install dlatk[wordcloud,langid]\n",
        "!dlatkInterface.py --colabify\n",
        "\n",
        "#We also ask colab to shorten the output rows so it's easier to scroll\n",
        "from dlatk.tools.colab_methods import colab_shorten_and_bg\n",
        "colab_shorten_and_bg() #keeps the output block short and changes the output background color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MgGVcZ_7ems"
      },
      "source": [
        "ðŸ‘† You should see `DLATK exits with success! A good day indeed  Â¯\\_(ãƒ„)_/Â¯.` if the install and colabify was complete.\n",
        "\n",
        "âœ‹ **NOTE** - You may also see a line reading \"ERROR: pip dependency ...\" but that is ok as long as you saw `DLATK exits with success!`.\n",
        "\n",
        "<br />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTXigvWwWWc9"
      },
      "source": [
        "<a name=\"data\"></a>\n",
        "# Setup data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: This setup data section is optional.** Read below if you wish to understand how to format data.\n",
        "\n",
        "<br />\n",
        "\n",
        "DLATK uses data in CSV (comma-seprated value) format. Your csv needs at least two columns:\n",
        "* **`message`**: contains the text to be analyzed.\n",
        "* **`message_id`**: contains a unique id for each message.\n",
        "\n",
        "<br />\n",
        "\n",
        "The CSV may also have other columns, including other ids you may wish to analyze your data by (e.g. `user_id`). For example,\n",
        "\n",
        "|message_id|message|user_id|created_date|\n",
        "|----------|-------|-------|------------|\n",
        "|17557|New rules on pizza to be introduced|1405024|2004-05-27|\n",
        "|15996|I just read John Kerry's nomination acceptance speech. Christ, he can talk.  God bless America.|3523319|2004-07-29|\n",
        "|27462| Talk about 'better late than never'...   urlLink  Couple Living Together 77 Years Marries.   Thanks to  Zorak  for the link.|942828|2003-04-04|\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "Now that you are familiar with the basic data format, you have two options:\n",
        "\n",
        "ðŸ‘‰ **1. If you wish to use the [default tutorial dataset](https://github.com/dlatk/dlatk/blob/public/dlatk/data/colab_dataset.md), then you can skip directly to the [Run Differential Language Analysis](#dla) section.** The default dataset contains the files mentioned below -\n",
        "* `msgs404u.csv` - language data from 404 blog authors.\n",
        "* `users404.csv` - age, gender, and occupation for the 404 authors.\n",
        "\n",
        "<br />\n",
        "\n",
        "ðŸ‘‰ **2. Read below if you wish to use your own data.**\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "qjiym3ZWIl-_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGFJq5uAv0q"
      },
      "source": [
        "## Setup data: Upload a custom CSV to Google Drive (Optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **If your data is not in Drive**, then you need to upload it by running the  `upload_dataset()` command as shown below. When you run it, click \"Allow\" for colab to access your drive. <br />\n",
        "<em>Again, you can also skip this optional step and jump ahead to \"[Run Differential Language Analysis (DLA)](#dla)\". You will use the default tutorial data, which is recommended for first time users.</em>\n",
        "\n",
        "<br/>\n",
        "\n",
        "* **If your data is already in Drive**, pass the file name to the command as an argument and your dataset will be downloaded to Colab. For example, if your file was in a folder named `DATA` and it was called `interesting_text.csv` then you would run `upload_dataset(\"interesting_text.csv\", \"DATA\")`\n"
      ],
      "metadata": {
        "id": "-kXHDdEVbyq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yyu24fpWV1E"
      },
      "outputs": [],
      "source": [
        "from dlatk.tools.colab_methods import upload_dataset\n",
        "upload_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† You should see `File FILENAME.csv copied successfully from Google Drive`.\n",
        "\n",
        "âœ‹ **NOTE** - If you get `MessageError: Error: credential propogation was unsuccessful` then you did not log in correctly.\n",
        "\n",
        "<hr />\n",
        "<br/>"
      ],
      "metadata": {
        "id": "eoBLVBXQEUnC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kO9FNS4hyO"
      },
      "source": [
        "**You are now ready to do some language analyses!**\n",
        "\n",
        "If your text data needs some cleaning, consider our [data cleaning tutorial](https://dlatk.github.io/dlatk/tutorials/tut_data_cleaning.html). It goes over, e.g., filtering to a language, removing common social media markup, and removing duplicates. For now it is not in colab, but all of the commands also work in colab.\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"dla\"></a>\n",
        "# Run Differential Language Analysis (DLA)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1TArhnhbdHThRL8_sebYBgkWkxZdQ9b_P\" height=\"350\" width=\"700\"/>\n",
        "\n",
        "<br/>\n",
        "\n"
      ],
      "metadata": {
        "id": "VqePY9RFEL5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dlatkInferface.py.** <br />\n",
        "<hr />\n",
        "\n",
        "The most popular way people use dlatk is through it's command interface: `dlatkInterface.py`.\n",
        "\n",
        "For all commands, DLATK uses the following **mandatory settings** -\n",
        "\n",
        ">  **`-d`** - **d**atabase that will contain all data dlatk work with. This is often just your project name. <br />\n",
        " **`-t`** - message **t**able here our text lives. If it's not loaded yet (as in this case), we can provide the name of the CSV file to be loaded into the database. <br />\n",
        " **`-g`** - **g**roup_id, the table column we will aggregate (unit of analysis).\n",
        "\n",
        " For this tutorial, all of your commands will begin with: `!dlatkInterface.py  -d colab_csv -t msgs404u.csv -g user_id` <br />\n",
        "> `colab_csv` is the **d**atabase, `msgs404u.csv` is the message_**t**able, and `user_id` is the **g**roup_id.\n",
        "\n",
        "\n",
        "<hr />\n",
        "<br />\n"
      ],
      "metadata": {
        "id": "kT_RYrYXlkye"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM8YgGtzAGb1"
      },
      "source": [
        "## DLA: Step 1 - Extract word and phrase (ngrams) features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step of DLA is to extract features from the language. Here, we do this by extracting words and 2-word phrases known as ngrams or \"tokens\" -- this process of splitting sequences of letters into words is called \"tokenization\".\n",
        "\n",
        "<br/>\n",
        "\n",
        "To do this, we use the following **ngram extraction flags** in addition to the mandatory settings.\n",
        "> **`--add_ngrams`** - the flag which starts the ngram extraction process<br />\n",
        " **`-n 1 [2] [3]`** -  the value or values for n in ngrams<br />\n",
        " **`--combine_feat_tables [NAME]`** - there will be 1 feature table for each `n`, this command concatenates them into one table with the provided `NAME`.\n",
        " <br />"
      ],
      "metadata": {
        "id": "DCkYVrHnyVfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together, this command will extract 1grams and 2grams from the `msgs404u.csv` file, and store them into a single \"feature table\" named `1to2gram`."
      ],
      "metadata": {
        "id": "KkaMhNmHNXDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GkIjJxvxfbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "2fb4bbfd-bdc2-4202-ca7f-5f3cc27c8da4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 250})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> body {background-color: rgb(255, 250, 232);}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-28 21:47:00\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/colab_csv.db\n",
            "query: PRAGMA table_info(msgs404u)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1gram$msgs404u$user_id\n",
            "SQL Query: CREATE TABLE feat$1gram$msgs404u$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(36), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$1gram$msgs404u$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$1gram$msgs404u$user_id ON feat$1gram$msgs404u$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$1gram$msgs404u$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$1gram$msgs404u$user_id ON feat$1gram$msgs404u$user_id (feat)\n",
            "query: PRAGMA table_info(msgs404u)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_1gram$msgs404u$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_1gram$msgs404u$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_1gram$msgs404u$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_1gram$msgs404u$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 404 'user_id's\n",
            "WARNING: The table msgs404u does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs404u does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 275 total ngram rows covering 1 user_ids\n",
            " [5%] Inserted 12717 total ngram rows covering 21 user_ids\n",
            " [10%] Inserted 23387 total ngram rows covering 41 user_ids\n",
            " [15%] Inserted 33730 total ngram rows covering 61 user_ids\n",
            " [20%] Inserted 44769 total ngram rows covering 81 user_ids\n",
            " [25%] Inserted 55626 total ngram rows covering 101 user_ids\n",
            " [30%] Inserted 66946 total ngram rows covering 122 user_ids\n",
            " [35%] Inserted 79582 total ngram rows covering 142 user_ids\n",
            " [40%] Inserted 92161 total ngram rows covering 162 user_ids\n",
            " [45%] Inserted 102162 total ngram rows covering 182 user_ids\n",
            " [50%] Inserted 112539 total ngram rows covering 202 user_ids\n",
            " [55%] Inserted 125751 total ngram rows covering 223 user_ids\n",
            " [60%] Inserted 136401 total ngram rows covering 243 user_ids\n",
            " [65%] Inserted 147041 total ngram rows covering 263 user_ids\n",
            " [70%] Inserted 161363 total ngram rows covering 283 user_ids\n",
            " [75%] Inserted 172518 total ngram rows covering 303 user_ids\n",
            " [80%] Inserted 183807 total ngram rows covering 324 user_ids\n",
            " [85%] Inserted 193257 total ngram rows covering 344 user_ids\n",
            " [90%] Inserted 207348 total ngram rows covering 364 user_ids\n",
            " [95%] Inserted 218394 total ngram rows covering 384 user_ids\n",
            " [100%] Inserted 229603 total ngram rows covering 404 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "query: PRAGMA table_info(msgs404u)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs404u$user_id\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs404u$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$2gram$msgs404u$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$2gram$msgs404u$user_id ON feat$2gram$msgs404u$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$2gram$msgs404u$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$2gram$msgs404u$user_id ON feat$2gram$msgs404u$user_id (feat)\n",
            "query: PRAGMA table_info(msgs404u)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_2gram$msgs404u$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_2gram$msgs404u$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_2gram$msgs404u$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_2gram$msgs404u$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 404 'user_id's\n",
            " [0%] Inserted 567 total ngram rows covering 1 user_ids\n",
            " [5%] Inserted 33034 total ngram rows covering 21 user_ids\n",
            " [10%] Inserted 60358 total ngram rows covering 41 user_ids\n",
            " [15%] Inserted 86148 total ngram rows covering 61 user_ids\n",
            " [20%] Inserted 113373 total ngram rows covering 81 user_ids\n",
            " [25%] Inserted 140966 total ngram rows covering 101 user_ids\n",
            " [30%] Inserted 168743 total ngram rows covering 122 user_ids\n",
            " [35%] Inserted 202100 total ngram rows covering 142 user_ids\n",
            " [40%] Inserted 233917 total ngram rows covering 162 user_ids\n",
            " [45%] Inserted 259040 total ngram rows covering 182 user_ids\n",
            " [50%] Inserted 284858 total ngram rows covering 202 user_ids\n",
            " [55%] Inserted 319678 total ngram rows covering 223 user_ids\n",
            " [60%] Inserted 346637 total ngram rows covering 243 user_ids\n",
            " [65%] Inserted 371996 total ngram rows covering 263 user_ids\n",
            " [70%] Inserted 411906 total ngram rows covering 283 user_ids\n",
            " [75%] Inserted 440185 total ngram rows covering 303 user_ids\n",
            " [80%] Inserted 467145 total ngram rows covering 324 user_ids\n",
            " [85%] Inserted 488606 total ngram rows covering 344 user_ids\n",
            " [90%] Inserted 524677 total ngram rows covering 364 user_ids\n",
            " [95%] Inserted 553607 total ngram rows covering 384 user_ids\n",
            " [100%] Inserted 581213 total ngram rows covering 404 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "Connecting to SQLite database: /content/sqlite_data/colab_csv.db\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1to2gram$msgs404u$user_id\n",
            "SQL Query: CREATE TABLE feat$1to2gram$msgs404u$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$1to2gram$msgs404u$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$1to2gram$msgs404u$user_id ON feat$1to2gram$msgs404u$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$1to2gram$msgs404u$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$1to2gram$msgs404u$user_id ON feat$1to2gram$msgs404u$user_id (feat)\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - colab_csv\n",
            "Corpus - msgs404u.csv\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$1to2gram$msgs404u$user_id\n",
            "-------\n",
            "Interface Runtime: 55.90 seconds\n",
            "DLATK exits with success! A good day indeed  Â¯\\_(ãƒ„)_/Â¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id \\\n",
        "  --add_ngrams -n 1 2 \\\n",
        "  --combine_feat_tables 1to2gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPtE75OJ4Bnf"
      },
      "source": [
        "ðŸ‘† If you see `DLATK exits with success!` then the command ran successfully and the features are ready!\n",
        "<hr />\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "\n",
        "To check the created feature tables, you can use the **`--show_feat_tables`** flag as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp3-4_wxEqrd"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id \\\n",
        "  --show_feat_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6DmMeoEzsb"
      },
      "source": [
        "ðŸ‘† You should see 5 feature tables:\n",
        "```\n",
        "feat$1gram$msgs404u$user_id\n",
        "feat$meta_1gram$msgs404u$user_id\n",
        "feat$2gram$msgs404u$user_id\n",
        "feat$meta_2gram$msgs404u$user_id\n",
        "feat$1to2gram$msgs404u$user_id\n",
        "```\n",
        "\n",
        "1.   `feat$1gram$msgs404u$user_id` contains the 1grams (single words).\n",
        "2.   `feat$meta_1gram$msgs404u$user_id` contains count data about 1grams.\n",
        "\n",
        "Similarly `feat$2gram$msgs404u$user_id` and `feat$meta_2gram$msgs404u$user_id` contain 2grams and their count data respectively, while `feat$1to2gram$msgs404u$user_id` is the concatenated table from `feat$1gram$msgs404u$user_id` and `feat$2gram$msgs404u$user_id`.\n",
        "\n",
        "<hr />\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr />\n",
        "\n",
        "So, what is a **Feature Table**? - A feature table is a data type to store extracted representations of the language. All feature tables contain at least the below columns -\n",
        "\n",
        "* `group_id` - individual units of analysis (group) of data\n",
        "* `feat` - extracted features, which can be tokens or lexicon features (to be discussed), embeddings, etc.\n",
        "* `value` - frequency value of the feature in the group.\n",
        "* `group_norm` - the normalized frequency of the feature (i.e. the relative frequency of the word) within the group.\n",
        "\n",
        "<hr />\n",
        "<br />"
      ],
      "metadata": {
        "id": "NbAUL3ZMH66f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can validate the feature table structure, and also their contents, by looking at the first 10 rows of feature tables using **`--view_tables`** flag:"
      ],
      "metadata": {
        "id": "uCJlfoRWReUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqJbap3uYFeD"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id \\\n",
        "  -f 'feat$1gram$msgs404u$user_id' \\\n",
        "  --view_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvkUxEHwAkse"
      },
      "source": [
        "ðŸ‘† Scroll and you should see the first 10 rows of the message table and the first 10 of the feature table.\n",
        "\n",
        "<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp0zRKd851Fh"
      },
      "source": [
        "## DLA: Step 2 - Feature Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of distinct tokens can be large in comparison to the sample size. However, a lot of these tokens are particular to just 1 or 2 examples (see [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) ), and not representative of the sample, and hence can be eliminated. Flags required to **filter such outlying words/phrases** are:\n",
        "\n",
        "> **`--feat_occ_filter`** - create a new feature table with infrequent features removed.<br />\n",
        " **`--set_p_occ`** - features used by less than this percentage of groups are dropped. <br />\n",
        " **`--feat_colloc_filter`** - create a new feature table without rare sequences of words (limiting to [collocations](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)).\n",
        "\n",
        "<br/>\n",
        "\n",
        "Similarly, when running filters, it is also good to specify a minimum word count per group (user in this case) required for the group to be considered:\n",
        "\n",
        "> **`--group_freq_thresh`** -  ignore groups which do not contain a certain number of words when running analyses (e.g. when calculating p_occ)"
      ],
      "metadata": {
        "id": "nc0KgyzXIKhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ For example, below command considers features (from `feat$1to2gram$msgs404u$user_id`) used by atleast `10%` of the groups, and groups with atleast `100` tokens."
      ],
      "metadata": {
        "id": "WWo6ifb68rO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMcCnrIT_hxZ"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  -f 'feat$1to2gram$msgs404u$user_id' \\\n",
        "  --feat_occ_filter --set_p_occ 0.10 --feat_colloc_filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MeYvx4560z"
      },
      "source": [
        "ðŸ‘† If you see `DLATK exits with success!`, then you have successfully filtered the features. The name of the filtered feature table - `feat$1to2gram$msgs404u$user_id$0_1$pmi3_0`.\n",
        "<hr />\n",
        "<br />\n",
        "\n",
        "To validate the structure and content of the created table, you can use `--view_tables` like above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avlIREmoGSUT"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id \\\n",
        "  -f 'feat$1to2gram$msgs404u$user_id$0_1$pmi3_0' \\\n",
        "  --view_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† You should see the first 10 rows of the `feat$1to2gram$msgs404u$user_id$0_1$pmi3_0` feature table.\n",
        "\n",
        "<br />\n",
        "<hr />\n",
        "<br />"
      ],
      "metadata": {
        "id": "0YX9JbMvd1XO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3Ps3VhAYdW"
      },
      "source": [
        "## DLA: Step 3 - Correlate (or associate) features with outcomes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have the language features, you can correlate them against outcomes like `age` while controlling for another variable like `gender`.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11ODkDFwsfQFc-5R5o757k3Ij9XbW9_Yb\" height=\"250\" width=\"500\"/>\n",
        "\n",
        "<br/>\n",
        "\n",
        "To do this, you need to tell DLATK the feature table to use and the outcomes:\n",
        "\n",
        "* **`-f 'TABLE_NAME'`** - names of the feature table (`feat$1to2gram$msgs404u$user_id$0_2$pmi3_0` in this case)\n",
        "* **`--outcome_table NAME`** - the name of the table with outcomes (`users404.csv`, can be the message table if it contains the outcomes)\n",
        "* **`--outcomes OC1 [OC2...]`** - list of outcomes to be associated with (`age` in this case)\n",
        "* **`--controls C1 [C2...]`** - list of statistical controls for the ascociation (we will control for `gender` in this example).\n",
        "\n",
        "<br/>\n",
        "\n",
        "Then, you specify what that you want a correlation matrix output with **`--rmatrix`** flag, and because the `gender` variable is categorical, we\n",
        " **`--cat_to_bin gender`** which converts the variable into [one-hot representation](https://wandb.ai/ayush-thakur/dl-question-bank/reports/How-One-Hot-Encoding-Improves-Machine-Learning-Performance--VmlldzoxOTkzMDk).\n"
      ],
      "metadata": {
        "id": "n0G8ppKjJCkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ For example, the command below associates the ngram features (`feat$1to2gram$msgs404u$user_id$0_1$pmi3_0`) with the `age` of the user, controlling for `gender`, and store the results into an HTML file, by default using [standardized multiple regression](link)."
      ],
      "metadata": {
        "id": "tABQYJNl9w6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFStIdMr4k-m"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --feat_table 'feat$1to2gram$msgs404u$user_id$0_1$pmi3_0' \\\n",
        "  --outcome_table users404.csv --outcomes age \\\n",
        "  --controls gender --cat_to_bin gender \\\n",
        "  --rmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† If you see `DLATK exits with success!` then the command executed successfully. The command should have stored an HTML (`feat.1to2gram.msgs404u.user_id.0_1.pmi3_0.age.gender__0.freq100.rMatrix.html`) with the results in the `Files` tab.\n",
        "\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PaP-OJH04D4ta-iV-nvpdel-K7oZyq8O\" height=\"300\" width=\"600\"/>\n",
        "\n",
        "<hr />\n",
        "<br />\n",
        "\n",
        "In addition to the tabular format, you can also observe the correlated features in the form of a word clouds. Flags required are **`--make_wordclouds`** (as shown below).\n",
        "\n"
      ],
      "metadata": {
        "id": "RSb9Pg1vHwrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oivrcA0_sTrK"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --feat_table 'feat$1to2gram$msgs404u$user_id$0_1$pmi3_0' \\\n",
        "  --outcome_table users404.csv --outcomes age \\\n",
        "  --controls gender --cat_to_bin gender \\\n",
        "  --rmatrix \\\n",
        "  --make_wordclouds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† The above command produces the HTML (as above) and wordclouds stored in the `feat.1to2gram.msgs404u.user_id.0_1.pmi3_0.age.gender__0.freq100._tagcloud_wordclouds` folder under the `Files` tab.\n",
        "\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_xiIFVjA6aYHcrnR8KJwKUdZaHdzWfQz\" height=\"300\" width=\"600\"/>\n",
        "\n",
        "<hr />\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "You can view the word clouds using the `print_wordclouds()` function below which takes in the path to the folder containing the word clouds. For example, to view the word clouds extracted earlier, the command would be - `print_wordclouds(\"feat.1to2gram.msgs404u.user_id.0_1.pmi3_0.age.gender__0.freq100._tagcloud_wordclouds\")`."
      ],
      "metadata": {
        "id": "6zrNcNVr5eAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dlatk.tools.colab_methods import print_wordclouds\n",
        "print_wordclouds(\"feat.1to2gram.msgs404u.user_id.0_1.pmi3_0.age.gender__0.freq100._tagcloud_wordclouds\")"
      ],
      "metadata": {
        "id": "oYvHVKiF00Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘†You should see two word clouds. The negative (words associated with younger age) and the positive (words and phrases associated with older age)."
      ],
      "metadata": {
        "id": "p6MHx1DKKpmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzy63dLCAT8N"
      },
      "source": [
        "## DLA: Using Other Features - Lexicon Features (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also generate language features from a lexicon (calculate the proportion of the lexicon present in the text) of your choice.\n",
        "\n",
        "Sometimes this is as simple as aggregating counts (in case of unweighted lexicon like LIWC) while sometimes there is a weighting factor involved (in case of weighted lexicon like `dd_permaV3` which measures an individual's well-being as per the PERMA scale).\n",
        "\n",
        "<br/>\n",
        "\n",
        "You need to provide the below flags in addition to mandatory settings -\n",
        "\n",
        "* **`--add_lex_table -l LEX_TABLE_NAME`** - commands DLATK to add the lexicon from `LEX_TABLE_NAME`\n",
        "* **`--weighted_lexicon` [Optional]** - flag to mention that the lexicon type is weighted."
      ],
      "metadata": {
        "id": "P9X7BifLKPoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ For example, you can extract features from *PERMA* dictionary as shown below."
      ],
      "metadata": {
        "id": "u9chxWx6VLnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iH5xg5oB_j5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "b3dd4148-8e30-46a4-af33-ccfb0890ba23"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 250})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> body {background-color: rgb(255, 250, 232);}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-28 21:46:41\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/colab_csv.db\n",
            "SQL Query: CREATE TABLE msgs404u (message_id INT, user_id INT, created_date VARCHAR(10), message LONGTEXT);\n",
            "Importing data, reading msgs404u.csv file\n",
            "Reading remaining 2392 rows into the table...\n",
            "query: PRAGMA table_info(msgs404u)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$cat_dd_permaV3_w$msgs404u$user_id$1gra\n",
            "SQL Query: CREATE TABLE feat$cat_dd_permaV3_w$msgs404u$user_id$1gra ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(10), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$cat_dd_permaV3_w$msgs404u$user_id$1gra, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$cat_dd_permaV3_w$msgs404u$user_id$1gra ON feat$cat_dd_permaV3_w$msgs404u$user_id$1gra (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$cat_dd_permaV3_w$msgs404u$user_id$1gra, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$cat_dd_permaV3_w$msgs404u$user_id$1gra ON feat$cat_dd_permaV3_w$msgs404u$user_id$1gra (feat)\n",
            "WORD TABLE feat$1gram$msgs404u$user_id\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/dlatkInterface.py\", line 2257, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/bin/dlatkInterface.py\", line 1078, in main\n",
            "    args.feattable = fe.addLexiconFeat(args.lextable, lowercase_only=args.lowercaseonly,\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/dlatk/featureExtractor.py\", line 2003, in addLexiconFeat\n",
            "    assert self.data_engine.tableExists(wordTable), \"Need to create word table to extract the lexicon: %s\" % wordTable\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: Need to create word table to extract the lexicon: feat$1gram$msgs404u$user_id\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --add_lex_table -l dd_permaV3 \\\n",
        "  --weighted_lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOEEHJWk6aov"
      },
      "source": [
        "ðŸ‘† The command executed successfully if `DLATK exits with success!`.\n",
        "\n",
        "<hr />\n",
        "\n",
        "<br />\n",
        "\n",
        "You should see the name of the new feature table - `feat$cat_dd_permaV3_w$msgs404u$user_id$1gra`, which you can validate like earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v42W5FupUbMs"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id \\\n",
        "  -f 'feat$cat_dd_permaV3_w$msgs404u$user_id$1gra' \\\n",
        "  --view_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† You should see the contents of `feat$cat_dd_permaV3_w$msgs404u$user_id$1gra`\n",
        "<hr />\n",
        "\n",
        "<br />\n",
        "\n",
        "Once you have represented your data samples in terms of lexicon features, you can also correlate them with outcomes, by changing the feature table in `--feat_table` (in this case `--feat_table 'feat$cat_dd_permaV3_w$msgs404u$user_id$1gra'`). Everything else in the command remains the same."
      ],
      "metadata": {
        "id": "EP7201RDGTe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, correlate the new set of features against `age` controlling for `gender`, like you did above."
      ],
      "metadata": {
        "id": "s1zPtayWVxpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --feat_table 'feat$cat_dd_permaV3_w$msgs404u$user_id$1gra' \\\n",
        "  --outcome_table users404.csv --outcomes age \\\n",
        "  --controls gender --cat_to_bin gender \\\n",
        "  --rmatrix"
      ],
      "metadata": {
        "id": "O18UHrEMoshJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† The above command produces an HTML (`feat.cat_dd_permaV3_w.msgs404u.user_id.1gra.age.gender__0.freq100.rMatrix.html`) with the results, under the `Files` tab.\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "kZ38XFF0GE9e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlLK0STHE9L"
      },
      "source": [
        "# Running Prediction (Language-based Assessment)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1AgMttURoLyJZ6WhL9wnVzWBt1gSmE0yE\" height=\"350\" width=\"550\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tACYr6g88H6n"
      },
      "source": [
        "## Prediction: Example 1: N-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of [prediction models](https://scikit-learn.org/stable/modules/linear_model.html#) we can run:\n",
        "1. **Regression**, which is the prediction of a continous variable, and\n",
        "2. **Classification**, which is the prediction of a categorical variable.\n",
        "\n",
        "<br/>\n",
        "\n",
        "<hr/>\n",
        "\n",
        "[N-Fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#) - This method involves randomly dividing the dataset into *N* groups, or \"folds\", of approximately equal size. The model is later fit on the N-1 folds (*train* data) and the evaluated for accuracy over the remaining one fold (*test* data).\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br/>\n",
        "\n",
        "For running a prediction model against an outcome, you use the same data setup commands as in correlations:\n",
        "* **`-f FEAT_TABLE_NAME`** - names of the feature table.\n",
        "* **`--outcome_table NAME`** - the name of the table with outcomes.\n",
        "* **`--outcomes OC1 [OC2...]`** - list of outcomes to predict.\n"
      ],
      "metadata": {
        "id": "PvDHsfsWQrb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Fold Regression:\n"
      ],
      "metadata": {
        "id": "xGa8-zHYKKtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the flags to perform regression using cross-validation:\n",
        "* **`--nfold_test_regression`** - this activates regression using n-fold cross-validation.\n",
        "* **`--model MODEL_NAME`** - which regression model to use (some examples would be ordinary least sqaures, ridge regression, etc.)"
      ],
      "metadata": {
        "id": "1UlH163FGfhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ For example, below command will predict `age` from 1gram and 2grams features using a ridge regression model over `5` fold cross validation."
      ],
      "metadata": {
        "id": "Y2x7ExfQA3dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrvew0R33AWA"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --outcome_table users404.csv --outcomes age \\\n",
        "  --feat_table 'feat$1to2gram$msgs404u$user_id$0_1' \\\n",
        "  --nfold_test_regression --model ridgecv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† Above the \"Settings\", you should see a dictionary containing accuracy metrics from cross validation. Key metrics are `r` and `mae`:\n",
        "```\n",
        "[TEST COMPLETE]\n",
        "\n",
        "{'age': {(): {1: {'N': 404,\n",
        "                  ...\n",
        "                  'mae': 5.392188704337425,\n",
        "                  'num_features': 2308,\n",
        "                  'r': 0.580652941321456,\n",
        "                  ...}}}}\n",
        "```\n",
        "* `mae` is the mean absolute error aggregated across all examples from when they were in a test fold. In this case, the model on average is off in predicting age by 5.4 years.\n",
        "\n",
        "* `r` is the Pearson correlation between the predicted age and the self-reported age. The correlation is a nice accuracy metric for regression prediction tasks  since it is bounded at 1 being a perfect prediction and 0 being what is expected by chance.  \n",
        "\n",
        "DLATK uses 5 folds by default, but you can change this by adding the\n",
        "`--folds K` parameter.\n",
        "\n",
        "<br/>\n"
      ],
      "metadata": {
        "id": "jz_xKX3neFC9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhOQOGiTaqW8"
      },
      "source": [
        "### N-Fold Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, you can also perform *classification* (i.e. [predicting categorical outcomes](https://en.wikipedia.org/wiki/Statistical_classification)) using cross-validation using the below flags:\n",
        "\n",
        "* **`--nfold_test_classifiers`** - activates classification using cross-validation.\n",
        "* **`--model MODEL_NAME`** - classification model to use (for example logistic regression, etc.).\n",
        "\n",
        "<br/>\n",
        "\n",
        "You can also store the prediction output to a CSV with the below flags:\n",
        "* **`--csv`** - Saves the results to a csv file instead of printing to the screen, like with `--correlate`.\n",
        "* **`--pred_csv`** - write the predicted scores for the sample to a separate CSV prefixed with the name in `--output_name`.\n",
        "\n",
        "<br/>\n",
        "\n",
        "ðŸ‘‡ Try predicting if a user is a student or not (`is_student`) from their 1gram and 2gram features using Logistic Regression (`lr`) in 5 fold cross-validation."
      ],
      "metadata": {
        "id": "SnsjxLReadU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCuCF14Eapmm"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --outcome_table users404.csv --outcomes is_student  \\\n",
        "  --feat_table 'feat$1to2gram$msgs404u$user_id$0_1' \\\n",
        "  --nfold_test_classifiers --model lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† The above command produce the key validation metrics like `acc` (accuracy) which gives the fraction of users correctly classified as student (`72.2%` in this case, as compared to a random or *most frequent class accuracy* (mfc_acc) of `56.1%`). Simple percentage acccuracy is not often the best metric for classification so it also includes [f1 score](https://en.wikipedia.org/wiki/F-score) and [auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
        "\n",
        "```\n",
        "[is_student]\n",
        "   NO CONTROLS\n",
        "     + LANG: acc: 0.722, f1: 0.708, auc: 0.745 (p_vs_controls = 1.0000)\n",
        "   (mfc_acc: 0.561)\n",
        "```\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "zWMd-qW-Toti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction - Example 2: Training and Deploying a model."
      ],
      "metadata": {
        "id": "WHBvEKsTRLm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Deploy - Step 1: Train and save a model\n"
      ],
      "metadata": {
        "id": "oNpyhU6XiA1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach is to train a predictive model, store it and use it to predict the outcomes in a different dataset.\n",
        "\n",
        "<br/>\n",
        "\n",
        "In addition to the flags that point to the right tables, flags necessary for this command are -\n",
        "\n",
        "* **`--train_regression`** - trains a regression model.\n",
        "* **`--model MODEL_NAME`** - which machine learning model to use.\n",
        "* **`--save_model --picklefile FILENAME`** - saves the model into a pickle file `FILENAME`\n",
        "\n",
        "<br/>\n",
        "\n",
        "ðŸ‘‡ In the below example, you will build and save a model (using [ridge regression](what_is_ridge) with a penalty of 1000) that predicts `age` from 1gram and 2gram features (`feat$1to2gram$msgs404u$user_id$0_1`), and save it into a file named `age.1to2grams.ridge1000.gft100.pickle`."
      ],
      "metadata": {
        "id": "bEyzbB5XaodD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs404u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --outcome_table users404.csv --outcomes age  \\\n",
        "  -f 'feat$1to2gram$msgs404u$user_id$0_1' \\\n",
        "  --train_regression --model ridge1000 \\\n",
        "  --save_model --picklefile age.1to2grams.ridge1000.gft100.pickle"
      ],
      "metadata": {
        "id": "-8LGlOOBp8W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Deploy - Step 2: Predict from new data\n",
        "\n"
      ],
      "metadata": {
        "id": "1ry-iqmWROil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have trained a model to predict an outcome, you can use it to predict from unseen data (or *test* data).\n",
        "\n",
        "ðŸ‘‰ If you wish to use a default *test* dataset, skip the next command.\n",
        "\n",
        "ðŸ‘‡ To load your dataset to test the model, you can use the `upload_dataset` function from the [Setup data](#data) section above."
      ],
      "metadata": {
        "id": "f4HkTFvrZOh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ONLY RUN THIS IF YOU WISH TO ADD NEW DATA\n",
        "from dlatk.tools.colab_methods import upload_dataset\n",
        "upload_dataset()"
      ],
      "metadata": {
        "id": "VFGe4TR9RSg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ You need to extract the same set of features as the ones used to train the predictive model. So, extract 1gram and 2gram features, used by atleast 10% of the groups, and consider only the groups with atleast 100 tokens."
      ],
      "metadata": {
        "id": "FWjIdHgYZw9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs100u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  --add_ngrams -n 1 2 \\\n",
        "  --combine_feat_tables 1to2gram \\\n",
        "  --feat_occ_filter --set_p_occ 0.10"
      ],
      "metadata": {
        "id": "hPHAMH9sMqKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† You should see the name of the new feature table - `feat$1to2gram$msgs100u$user_id$0_1$pmi3_0` (you can use `--view_tables` to check the table).\n",
        "<hr /><br />\n",
        "\n"
      ],
      "metadata": {
        "id": "Ihi0766mfTx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have all the features required by the prediction model, you can predict the outcome using the flags:\n",
        "\n",
        "* **`--predict_regression_to_outcome_table TABLE_NAME`** - predicts the outcomes into a table named `TABLE_NAME`.\n",
        "* **`--load --picklefile FILENAME`** - load the model from the file `FILENAME`."
      ],
      "metadata": {
        "id": "fFiIGzTXf2Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dlatkInterface.py \\\n",
        "  -d colab_csv -t msgs100u.csv -g user_id --group_freq_thresh 100 \\\n",
        "  -f 'feat$1to2gram$msgs100u$user_id$0_1' \\\n",
        "  --predict_regression_to_outcome_table lbp_age \\\n",
        "  --load --picklefile age.1to2grams.ridge1000.gft100.pickle"
      ],
      "metadata": {
        "id": "3z0FH0p3L1L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘† You should see the name of the new table with the predictions - `p_ridg$lbp_age`. You should also see the top-3 messages and predicted age for 2 users picked at random. For example -\n",
        "\n",
        "```\n",
        "...\n",
        "Example predictions\n",
        "-------\n",
        "Group ID: 4088894\n",
        "Top 3 messages for the group:\n",
        "I have been a member of the Dragonswood site...\n",
        "\n",
        "Well, this weekend is going to me quadrupally special for me...\n",
        "\n",
        "Well, I've never run a blog before...\n",
        "\n",
        "Prediction: 27.350689449787602\n",
        "-------\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "IgGPXbDEA9pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authors and References\n",
        "\n",
        "* [Shashanka Subrahmanya](https://github.com/shshnk94) - Lead Colab Developer\n",
        "* [H. Andrew Schwartz](https://www3.cs.stonybrook.edu/~has/) - DLATK Creator / Colab Mentor\n",
        "* [Johannes Eichstaedt](https://jeichstaedt.com/) - Mentor\n",
        "* [Adithya Ganesan](https://sjgiorgi.github.io/) - DLATK Developer\n",
        "* [Salvatore Giorgi](https://sjgiorgi.github.io/) - DLATK Maintainer\n",
        "\n",
        "### References:\n",
        "**Colab:** <br />\n",
        "Subrahmanya, S., Schwartz, H. A., Eichstaedt, J. C., Ganesan, A., & Giorgi, S. (2024). DLATK - Colab Tutorial for Differential Language Analysis. [*github.com/dlatk/dlatk/blob/public/colab.md.*](https://github.com/dlatk/dlatk/blob/public/colab.md)\n",
        "\n",
        "**DLATK:** <br />\n",
        "Schwartz, H. A., Giorgi, S., Sap, M., Crutchley, P., Ungar, L., & Eichstaedt, J. (2017). Dlatk: Differential language analysis toolkit. In *Proceedings of the 2017 conference on empirical methods in natural language processing: System demonstrations* (pp. 55-60)."
      ],
      "metadata": {
        "id": "i43f_1bAFIDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sqlite3 /content/sqlite_data/colab_csv.db"
      ],
      "metadata": {
        "id": "B_OuwVWdQVUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7bP-FbmOklP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lTXigvWwWWc9",
        "YM8YgGtzAGb1",
        "tp0zRKd851Fh",
        "ib3Ps3VhAYdW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}